{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XFLSIX_pMb-7",
        "outputId": "5aaa7227-37c5-4408-a3e2-2ed0218ab098"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZ7_zv-PvUs",
        "outputId": "3b4a635a-97a3-4282-dd52-3357ed167f1b"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & IMPORTS\n",
        "# ==========================================\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import cv2\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# ==========================================\n",
        "# 2. AUTHENTICATION (SERVICE ACCOUNT)\n",
        "# ==========================================\n",
        "# Path to your uploaded key file\n",
        "# NOTE: Ensure this file is uploaded to your current folder in Workbench!\n",
        "KEY_FILE = 'colab-upload-bot-key.json' \n",
        "\n",
        "if os.path.exists(KEY_FILE):\n",
        "    print(\"üîê Authenticating with Service Account Key...\")\n",
        "    os.system(f'gcloud auth activate-service-account --key-file=\"{KEY_FILE}\"')\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è WARNING: Key file '{KEY_FILE}' not found.\")\n",
        "    print(\"   If you are already logged in to gcloud on this VM, you can ignore this.\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. DOWNLOAD DATA (RELATIVE PATHS)\n",
        "# ==========================================\n",
        "# GCS Configuration\n",
        "BUCKET_NAME = 'vis-data-2025'\n",
        "GCS_TRAIN_DIR = f'gs://{BUCKET_NAME}/trainsm'  \n",
        "GCS_JSON_URL = f'gs://{BUCKET_NAME}/train.json'\n",
        "\n",
        "# --- FIXED PATHS (Relative to where you run the notebook) ---\n",
        "LOCAL_BASE_DIR = './data_local'\n",
        "LOCAL_TRAIN_DIR = os.path.join(LOCAL_BASE_DIR, 'trainsm')\n",
        "LOCAL_JSON_PATH = './train.json'\n",
        "\n",
        "def download_with_progress(gcs_src, local_dest_folder):\n",
        "    \"\"\"Counts files and downloads with a progress bar.\"\"\"\n",
        "    try:\n",
        "        # Create destination if it doesn't exist\n",
        "        os.makedirs(local_dest_folder, exist_ok=True)\n",
        "\n",
        "        # 1. Count files\n",
        "        print(f\"üîç Counting files in {gcs_src}...\")\n",
        "        count_cmd = f\"gsutil ls -r {gcs_src} | wc -l\"\n",
        "        result = subprocess.run(count_cmd, shell=True, capture_output=True, text=True)\n",
        "        try:\n",
        "            total_files = int(result.stdout.strip())\n",
        "        except ValueError:\n",
        "            total_files = 1000 # Fallback if count fails\n",
        "\n",
        "        print(f\"üì¶ Found ~{total_files} files. Starting download to {local_dest_folder}...\")\n",
        "        \n",
        "        # 2. Start Download\n",
        "        # Note: We copy the CONTENTS of trainsm into local_dest_folder\n",
        "        # If gcs_src is .../trainsm, gsutil cp -r gs://.../trainsm ./data_local/ \n",
        "        # will create ./data_local/trainsm\n",
        "        \n",
        "        # We target the PARENT directory so gsutil creates the 'trainsm' folder inside it\n",
        "        parent_dir = os.path.dirname(local_dest_folder)\n",
        "        \n",
        "        process = subprocess.Popen(\n",
        "            f'gsutil -m cp -r {gcs_src} {parent_dir}',\n",
        "            shell=True,\n",
        "            stderr=subprocess.PIPE, \n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        pbar = tqdm(total=total_files, unit='file', desc='Downloading')\n",
        "        for line in process.stderr:\n",
        "            if \"Copying\" in line:\n",
        "                pbar.update(1)\n",
        "        \n",
        "        process.wait()\n",
        "        pbar.close()\n",
        "        \n",
        "        if process.returncode == 0:\n",
        "            print(\"‚úÖ Download Complete.\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå Download command failed.\")\n",
        "            # Print last few lines of error to help debug\n",
        "            print(\"Last error output:\", line) \n",
        "            return False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during download: {e}\")\n",
        "        return False\n",
        "\n",
        "print(f\"\\nüöÄ STARTING DATA SETUP...\")\n",
        "\n",
        "# A. Download Annotation JSON\n",
        "if not os.path.exists(LOCAL_JSON_PATH):\n",
        "    print(f\"‚¨áÔ∏è Downloading annotations...\")\n",
        "    os.system(f'gsutil cp {GCS_JSON_URL} {LOCAL_JSON_PATH}')\n",
        "else:\n",
        "    print(\"‚úÖ Annotations already present.\")\n",
        "\n",
        "# B. Download Training Folder\n",
        "if not os.path.exists(LOCAL_TRAIN_DIR):\n",
        "    # Ensure base dir exists\n",
        "    os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
        "    download_with_progress(GCS_TRAIN_DIR, LOCAL_TRAIN_DIR)\n",
        "else:\n",
        "    print(\"‚úÖ Training data already exists locally.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. CONFIGURATION\n",
        "# ==========================================\n",
        "TRAIN_DIR = LOCAL_TRAIN_DIR\n",
        "ANNOTATION_FILE = LOCAL_JSON_PATH\n",
        "OUTPUT_CSV_PATH = './metrics/baseline_gpu.csv' # Saved in current folder\n",
        "\n",
        "MODEL_NAME = 'yolov5l6'\n",
        "IMG_SIZE = 1280\n",
        "CONF_THRESH = 0.05\n",
        "IOU_THRESH = 0.45\n",
        "\n",
        "print(f\"‚è≥ Loading High-Res Model: {MODEL_NAME}...\")\n",
        "try:\n",
        "    model = torch.hub.load('ultralytics/yolov5', MODEL_NAME, pretrained=True, force_reload=False)\n",
        "    model.conf = CONF_THRESH\n",
        "    model.classes = [14]  # Class 14 = Bird\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(\"‚úÖ Model Loaded on GPU.\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"‚ö†Ô∏è Model Loaded on CPU.\")\n",
        "        \n",
        "    model.to(device)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model Load Error: {e}\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "def load_json_ground_truth(json_path):\n",
        "    if not os.path.exists(json_path):\n",
        "        print(f\"‚ùå Error: Annotation file not found at {json_path}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"üìÇ Parsing annotations from: {json_path}...\")\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå JSON Parse Error: {e}\")\n",
        "        return {}\n",
        "\n",
        "    id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n",
        "    img_id_to_boxes = defaultdict(list)\n",
        "    if 'annotations' in data:\n",
        "        for ann in data['annotations']:\n",
        "            img_id_to_boxes[ann['image_id']].append(ann['bbox'])\n",
        "\n",
        "    filename_to_gt = {}\n",
        "    for img_id, filename in id_to_filename.items():\n",
        "        key = filename\n",
        "        if key.startswith('train/'):\n",
        "            key = key.replace('train/', '', 1)\n",
        "        filename_to_gt[key] = img_id_to_boxes.get(img_id, [])\n",
        "\n",
        "    print(f\"‚úÖ Loaded GT for {len(filename_to_gt)} images.\")\n",
        "    return filename_to_gt\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    xi1 = max(x1, x2)\n",
        "    yi1 = max(y1, y2)\n",
        "    xi2 = min(x1 + w1, x2 + w2)\n",
        "    yi2 = min(y1 + h1, y2 + h2)\n",
        "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    union_area = (w1 * h1) + (w2 * h2) - inter_area\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "    \n",
        "def get_next_version_path(path):\n",
        "    \"\"\"\n",
        "    Returns a new file path with an incremented version number if the file already exists.\n",
        "    Example: 'data.csv' -> 'data_1.csv' -> 'data_2.csv'\n",
        "    \"\"\"\n",
        "    # If the file doesn't exist yet, simply return the original path\n",
        "    if not os.path.exists(path):\n",
        "        return path\n",
        "\n",
        "    directory, filename = os.path.split(path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    \n",
        "    # Create the directory if it doesn't exist (safety check)\n",
        "    if directory and not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Regex pattern to match files like \"baseline_gpu_123.csv\"\n",
        "    # Matches: exact_name + underscore + digits + exact_extension\n",
        "    pattern = re.compile(rf\"^{re.escape(name)}_(\\d+){re.escape(ext)}$\")\n",
        "    \n",
        "    max_version = 0\n",
        "    \n",
        "    # List files in the directory to find the highest existing number\n",
        "    for f in os.listdir(directory if directory else '.'):\n",
        "        match = pattern.match(f)\n",
        "        if match:\n",
        "            version = int(match.group(1))\n",
        "            if version > max_version:\n",
        "                max_version = version\n",
        "\n",
        "    # Next version is max found + 1\n",
        "    new_filename = f\"{name}_{max_version + 1}{ext}\"\n",
        "    return os.path.join(directory, new_filename)\n",
        "    \n",
        "# ==========================================\n",
        "# 6. MAIN PIPELINE\n",
        "# ==========================================\n",
        "def run_baseline_evaluation():\n",
        "    gt_data = load_json_ground_truth(ANNOTATION_FILE)\n",
        "    if not gt_data: return\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Check for trainsm folder contents\n",
        "    video_folders = sorted(glob.glob(os.path.join(TRAIN_DIR, '*')))\n",
        "    video_folders = [f for f in video_folders if os.path.isdir(f)]\n",
        "\n",
        "    if not video_folders:\n",
        "        print(f\"‚ùå No video folders found in {TRAIN_DIR}.\")\n",
        "        print(f\"   (Current working dir: {os.getcwd()})\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìÇ Found {len(video_folders)} videos locally.\")\n",
        "    \n",
        "    total_tp = total_fp = total_fn = total_time_sec = total_frames = 0\n",
        "    results_data = []\n",
        "\n",
        "    print(f\"\\n{'Video':<10} | {'Frames':<6} | {'FPS':<6} | {'Prec':<6} | {'Recall':<6} | {'F1':<6}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for v_idx, video_path in enumerate(video_folders):\n",
        "        video_name = os.path.basename(video_path)\n",
        "        images = sorted(glob.glob(os.path.join(video_path, '*.jpg')))\n",
        "\n",
        "        if not images: continue\n",
        "\n",
        "        vid_tp = vid_fp = vid_fn = 0\n",
        "        vid_start = time.time()\n",
        "        n_frames = len(images)\n",
        "\n",
        "        for i, img_path in enumerate(images):\n",
        "            # Progress Bar for Frames\n",
        "            if i % 10 == 0:\n",
        "                percent = ((i + 1) / n_frames) * 100\n",
        "                sys.stdout.write(f\"\\rüëâ Processing [{video_name}] Frame {i+1}/{n_frames} ({percent:.1f}%)\")\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            img_filename = os.path.basename(img_path)\n",
        "            lookup_key = f\"{video_name}/{img_filename}\"\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None: continue\n",
        "\n",
        "            results = model(img, size=IMG_SIZE)\n",
        "            preds = []\n",
        "            results_numpy = results.xyxy[0].cpu().numpy() if torch.cuda.is_available() else results.xyxy[0].numpy()\n",
        "            for det in results_numpy:\n",
        "                x1, y1, x2, y2, conf, cls = det\n",
        "                preds.append([x1, y1, x2-x1, y2-y1])\n",
        "\n",
        "            gts = gt_data.get(lookup_key, [])\n",
        "            matched_gt = set()\n",
        "\n",
        "            for p_box in preds:\n",
        "                best_iou = 0\n",
        "                best_gt_idx = -1\n",
        "                for idx, g_box in enumerate(gts):\n",
        "                    if idx in matched_gt: continue\n",
        "                    iou = calculate_iou(p_box, g_box)\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_gt_idx = idx\n",
        "\n",
        "                if best_iou >= IOU_THRESH:\n",
        "                    vid_tp += 1\n",
        "                    matched_gt.add(best_gt_idx)\n",
        "                else:\n",
        "                    vid_fp += 1\n",
        "            vid_fn += len(gts) - len(matched_gt)\n",
        "\n",
        "        vid_end = time.time()\n",
        "        vid_time = vid_end - vid_start\n",
        "        vid_fps = n_frames / vid_time if vid_time > 0 else 0\n",
        "\n",
        "        total_time_sec += vid_time\n",
        "        total_frames += n_frames\n",
        "        total_tp += vid_tp\n",
        "        total_fp += vid_fp\n",
        "        total_fn += vid_fn\n",
        "\n",
        "        prec = vid_tp / (vid_tp + vid_fp) if (vid_tp + vid_fp) > 0 else 0\n",
        "        rec = vid_tp / (vid_tp + vid_fn) if (vid_tp + vid_fn) > 0 else 0\n",
        "        f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
        "\n",
        "        sys.stdout.write(\"\\r\" + \" \" * 80 + \"\\r\")\n",
        "        print(f\"{video_name:<10} | {n_frames:<6} | {vid_fps:<6.1f} | {prec:<6.2f} | {rec:<6.2f} | {f1:<6.2f}\")\n",
        "\n",
        "        results_data.append({\n",
        "            'Video': video_name,\n",
        "            'Frames': n_frames,\n",
        "            'FPS': round(vid_fps, 2),\n",
        "            'Precision': round(prec, 4),\n",
        "            'Recall': round(rec, 4),\n",
        "            'F1': round(f1, 4),\n",
        "            'TP': vid_tp, 'FP': vid_fp, 'FN': vid_fn\n",
        "        })\n",
        "\n",
        "    print(\"=\" * 65)\n",
        "    avg_fps = total_frames / total_time_sec if total_time_sec > 0 else 0\n",
        "    overall_prec = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
        "    overall_rec = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
        "    overall_f1 = 2 * (overall_prec * overall_rec) / (overall_prec + overall_rec) if (overall_prec + overall_rec) > 0 else 0\n",
        "\n",
        "    print(\"FINAL RESULTS (Small Subset):\")\n",
        "    print(f\"Total Frames:   {total_frames}\")\n",
        "    print(f\"Average FPS:    {avg_fps:.2f}\")\n",
        "    print(f\"Precision:      {overall_prec:.4f}\")\n",
        "    print(f\"Recall:         {overall_rec:.4f}\")\n",
        "    print(f\"F1-Score:       {overall_f1:.4f}\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    df = pd.DataFrame(results_data)\n",
        "    overall_row = {\n",
        "        'Video': 'OVERALL', 'Frames': total_frames, 'FPS': round(avg_fps, 2),\n",
        "        'Precision': round(overall_prec, 4), 'Recall': round(overall_rec, 4),\n",
        "        'F1': round(overall_f1, 4), 'TP': total_tp, 'FP': total_fp, 'FN': total_fn\n",
        "    }\n",
        "    df = pd.concat([df, pd.DataFrame([overall_row])], ignore_index=True)\n",
        "    final_path = get_next_version_path(OUTPUT_CSV_PATH)\n",
        "    df.to_csv(final_path, index=False)\n",
        "    print(f\"‚úÖ CSV Saved: {final_path}\")\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"‚è±Ô∏è Process took: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_baseline_evaluation()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
